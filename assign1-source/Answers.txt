QUESTIONS:

1 - Implement:
  a. - The perceptron learning algorithm
	   For PERCEPTRON: Training loss = 7.1637486599753, error = 0.191; Testing loss = 7.2037182760096, error = 0.165

  b. - Linear regression with square loss trained with direction solution and 
       stochastic gradient descent
	   
       DIRECT SOLUTION: 

	   Stochastic Gradient Descent: 

  c. - The Logistic regression algorithm trained with stochastic gradient
       descent
  d. - The multinomial Logistic regression algorithm trained with stochastic
       gradient descent

  ==> Include your code as attachment of an email to the TA.

2 - Experiments with the Spambase dataset
     - Set the training set size to 1000 and the 				test set to 1000.
     - Use the (ridge) l2 regularization
     - The stochastic gradient method for linear regression and logistic
       regression requires you to find good values for the learning rate
       (the step size eta using stepCons)
  a. - What learning rate will cause linear regression and logistic regression
       to diverge?
ANSWER:
2.a. Learning rate causing linear regression to diverge:
     Learning rate causing logistic regression to diverge:

  b. - What learnin rate for linear regression and logistic regression that
       produces the fastest convergence?
ANSWER:
2.b.
	 Learning rate causing linear regression for fastest convergence:
     Learning rate causing logistic regression for fastest convergence:

  c. - Implement a stopping criterion that detects convergence.
ANSWER:
2.c.
 (Actual implementation is found in trainer.lua)

-- Code for stopping criteria. Let it run for 20 iterations at least. Then compare with last 10 iterations to see if loss has not changed much.
     local can_be_stopped = true
     prev_losses[i]=loss2

     if (i>10) then
		 for j=i-9,i do
		 	if (torch.abs(prev_losses[j]-prev_losses[j-1])<0.005) then
		 		local does_nothing=true
		 	else can_be_stopped=false
		 	end
		 end
     end
     
	 if i>10 then
		 if (can_be_stopped==true) then
		 print("Convergence detected, so stopping, after "..i.." iterations performed.")
		 break 
		 end
     end


  d. - Train logistic regression with 10, 30, 100, 500, 1000 and 3000 trianing
       samples, and 1000 test samples. For each size of the training set,
       provide:
       - The final value of the average loss, and the classification errors on
         the training set and the test set
       - The value of the learning rate used
       - The number of iterations performed
ANSWER:
2.d.
      let us first try the case where we keep all Lambda = 0.05, then

Testing all against SAME Test Set......

10 max_steps Logistic regression convergence testing:  -- NO CONVERGENCE
------------------------------

30 max_steps Logistic regression convergence testing:  -- NO CONVERGENCE
------------------------------

100 max_steps Logistic regression convergence testing: -- NO CONVERGENCE
------------------------------

500 max_steps Logistic regression convergence testing:
Convergence detected, so stopping, after 227 iterations performed.
------------------------------

1000 max_steps Logistic regression convergence testing:
Convergence detected, so stopping, after 227 iterations performed.
------------------------------

3000 steps Logistic regression convergence testing:
Convergence detected, so stopping, after 227 iterations performed.
------------------------------

10 FOR LOGISTIC REGRESSION with l2 reg: Training loss = 3.087106673221, error = 0.1; Testing loss = inf, error = 0.454
30 FOR LOGISTIC REGRESSION with l2 reg: Training loss = 2.3146480299949, error = 0.26666666666667; Testing loss = 21.379858833164, error = 0.42
100 FOR LOGISTIC REGRESSION with l2 reg: Training loss = 1.3836411232497, error = 0.13; Testing loss = 15.266060181732, error = 0.34
500 FOR LOGISTIC REGRESSION with l2 reg: Training loss = 2.2738270573943, error = 0.238; Testing loss = 9.449262113937, error = 0.296
1000 FOR LOGISTIC REGRESSION with l2 reg: Training loss = 3.0208927811382, error = 0.249; Testing loss = 9.3931810715075, error = 0.259
3000 FOR LOGISTIC REGRESSION with l2 reg: Training loss = 3.1058976649686, error = 0.227; Testing loss = 10.026898746481, error = 0.264

      So now we will use different different lambdas for different max_steps (no. of iterations)

For 10 samples:
   FOR LOGISTIC REGRESSION with l2 reg: Training loss = 2.8347902814814, error = 0.1; Testing loss = 10.246736493107, error = 0.397
   Learning rate:0.0000001 - 0.1
   Number of iterations:NO CONVERGENCE For any Lambda.

For 30 samples:
   FOR LOGISTIC REGRESSION with l2 reg: Training loss = 8.2057731060219, error = 0.2; Testing loss = 15.437865853392, error = 0.426
   Learning rate:0.0000001 - 0.1
   Number of iterations:NO CONVERGENCE For any Lambda.

For 100 samples:
   FOR LOGISTIC REGRESSION with l2 reg: Training loss = 9.1528290811417, error = 0.13; Testing loss = 12.304956364362, error = 0.178
   Learning rate:0.0000001 - 0.1
   Number of iterations: NO CONVERGENCE For any Lambda.

For 500 samples:
   FOR LOGISTIC REGRESSION with l2 reg: Training loss = 2.6992460737131, error = 0.194; Testing loss = 2.9414021066646, error = 0.153
   Learning rate:0.05
   Number of iterations:Convergence detected, so stopping, after 463 iterations performed.

For 1000 samples:
   FOR LOGISTIC REGRESSION with l2 reg: Training loss = 1.6921011590229, error = 0.11; Testing loss = 2.2107017685683, error = 0.129
   Learning rate:0.03
   Number of iterations:Convergence detected, so stopping, after 327 iterations performed.

For 3000 samples:
   FOR LOGISTIC REGRESSION with l2 reg: Training loss = 1.5286116898631, error = 0.13666666666667; Testing loss = 2.0640244582992, error = 0.185
   Learning rate:0.01
   Number of iterations:Convergence detected, so stopping, after 350 iterations performed.

	**Observations:
		1. It seems very hard to get convergence on 10, 30, 100 training sample size experiments.
		2. No. of iterations to converge 1000 seems to be lesser than 500 as well as 3000 training sample size experiments.


  e. - What is the asymptotic value of the training/test error for very large
       training sets?
	When using no. of training sets as 3000
    FOR LOGISTIC REGRESSION with l2 reg: Training loss = 2.6176745240151, error = 0.14066666666667; Testing loss = 2.5657795621824, error = 0.151


3. - L2 and L1 regularization
     When the training set is small, it is often helpful to add a
     regularization term to the loss function. The most popular ones are:
     L2 Norm: lambda*||W||^2 (aka "Ridge")
     L1 Norm: lambda*[\sum_{i} |W_i|] (aka "LASSO")
  a. - How is the linear regression with direct solution modified by the
       addition of an L2 regularizer?

 ANSWER:
 
3.a

For Linear regression with Direct solution with l2 loss = 0.22380390782004, error = 0.11133333333333;
For Linear regression with Direct solution with l1 loss = 0.23842201248454, error = 0.11133333333333;

For Linear regression with Direct solution with l2 loss = 0.22918331079427, error = 0.11233333333333;
For Linear regression with Direct solution with l1 loss = 0.24600017475571, error = 0.11233333333333;

In our direct solution: we use trainer:test() to calculate our loss and error. And, in trainer:test() to calculate loss and error it uses, model:l() to calculate 
loss and model:g() to calculate error. And, model:l() has a regularization term whereas model:g() does not have a regularization term. So, while using l2 and l1 
regularizers, we get different different losses but the same errors.


b. - Implement the L1 regularizer. Experiment with your logistic regression
       code with the L2 and L1 regularizers. Can you improve the performance on
       the test set for training set sizes of 10, 30 and 100? What value of
       lambda gives the best results?

 ANSWER:

3.b


For LOGISTIC REGRESSION:

For training set size 10:
Lambda = 0.03
	l2 reg: Training loss = 0.70409910052754, error = 0; Testing loss = 11.551727932773, error = 0.393
	l1 reg: Training loss = 0.97635904674392, error = 0; Testing loss = 17.207432287684, error = 0.398
Lambda = 0.05
	l2 reg: Training loss = 1.1378042893368, error = 0; Testing loss = 17.370828612985, error = 0.305
	l1 reg: Training loss = 1.5512243611654, error = 0; Testing loss = 12.456012440083, error = 0.396
Lambda = 0.07
	l2 reg: Training loss = 1.4355104508443, error = 0; Testing loss = 24.274573882386, error = 0.392
	l1 reg: Training loss = 2.3993895847476, error = 0.1; Testing loss = 20.42464366037, error = 0.424
Lambda = 0.09
	l2 reg: Training loss = 1.8955248266468, error = 0; Testing loss = 13.065804724883, error = 0.344
	l1 reg: Training loss = 2.8911038407556, error = 0; Testing loss = 17.503997356674, error = 0.349

For training set size 30:
Lambda = 0.03
	l2 reg: Training loss = 1.30310509042, error = 0.066666666666667; Testing loss = 14.986124931949, error = 0.334
	l1 reg: Training loss = 1.2655169724744, error = 0.1; Testing loss = 13.369678029342, error = 0.258
Lambda = 0.05
	l2 reg: Training loss = 1.4333043233448, error = 0.1; Testing loss = 16.817180342136, error = 0.369
	l1 reg: Training loss = 2.0738462532878, error = 0.1; Testing loss = 12.106682965486, error = 0.351
Lambda = 0.07
	l2 reg: Training loss = 1.7324295998115, error = 0.1; Testing loss = 12.823186325126, error = 0.304
	l1 reg: Training loss = 2.2411218017041, error = 0.1; Testing loss = 15.633740190863, error = 0.321
Lambda = 0.09
	l2 reg: Training loss = 2.0925329886924, error = 0.1; Testing loss = 12.455692123079, error = 0.345
	l1 reg: Training loss = 3.0258982229099, error = 0.066666666666667; Testing loss = 12.130551221741, error = 0.344

For training set size 100:
Lambda = 0.03
	l2 reg: Training loss = 1.4462434680174, error = 0.11; Testing loss = 3.537803720041, error = 0.15
	l1 reg: Training loss = 1.3287434174105, error = 0.15; Testing loss = 3.1388047670939, error = 0.163
Lambda = 0.05
	l2 reg: Training loss = 1.3166491858152, error = 0.13; Testing loss = 3.2620797466237, error = 0.151
	l1 reg: Training loss = 1.3008585898619, error = 0.1; Testing loss = 3.2024011997136, error = 0.158
Lambda = 0.07
	l2 reg: Training loss = 1.553005288386, error = 0.09; Testing loss = 3.0505442189142, error = 0.16
	l1 reg: Training loss = 1.0612471682402, error = 0.16; Testing loss = 2.6408889338515, error = 0.193
Lambda = 0.09
	l2 reg: Training loss = 1.3890063587835, error = 0.12; Testing loss = 2.5589154658126, error = 0.155
	l1 reg: Training loss = 0.60347011055328, error = 0.1; Testing loss = 3.255872152184, error = 0.186




4. - Multinomial logistic regression
     Implement the multinomial logistic regression as a model. Your can
     experiment with larger datasets if your machine has enough memory. In this
     part we experiment on the MNIST dataset.
  a. - Testing your model with training sets of size 100, 500, 1000, and 6000,
       with a testing set of size 1000. What are the error rates on the
       training and testing datasets?
  b. - Use both L2 and L1 regularization. Do they make some difference?
ANSWER:

